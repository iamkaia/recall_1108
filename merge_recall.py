#!/usr/bin/env python3
# merge_recall.py â€” RECALL (layer-wise similarity + softmax merge, with SVD)
# Kaia final

import os
import re
import json
import argparse
from typing import List, Dict

import torch
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# ---------------------------
# Utils
# ---------------------------
def get_layer_id_from_name(name: str) -> int:
    """
    å˜—è©¦å¾ module åç¨±æŠ“å‡º transformer å±¤è™Ÿã€‚
    ä¾‹å¦‚ï¼š...model.layers.23.self_attn.q_proj...
    æ‰¾ä¸åˆ°å°±å› -1ï¼ˆä¹‹å¾Œæœƒç”¨å‡å€¼æ¬Šé‡åšå¾Œå‚™ï¼‰ã€‚
    """
    m = re.search(r"layers\.(\d+)\.", name)
    return int(m.group(1)) if m else -1


def load_jsonl(path: str) -> List[Dict]:
    if not os.path.exists(path):
        return []
    with open(path, "r", encoding="utf-8") as f:
        return [json.loads(line) for line in f]


def sample_prompts_from_tasks(data_root: str, tasks: List[str], k_per_task: int) -> List[str]:
    prompts = []
    for t in tasks:
        # å„ªå…ˆç”¨ trainï¼Œæ²’æœ‰å°± test
        train_p = os.path.join(data_root, t, "train.jsonl")
        test_p  = os.path.join(data_root, t, "test.jsonl")
        data = load_jsonl(train_p) or load_jsonl(test_p)
        for ex in data[:k_per_task]:
            inp = ex.get("input", "").strip()
            if inp:
                prompts.append(inp + "\nAnswer:")
    # è¬ä¸€è³‡æ–™å¤¾æ˜¯ç©ºçš„ï¼Œå°±çµ¦å¹¾æ¢ä¿åº• prompt
    if not prompts:
        prompts = [
            "State one advantage of convolutional neural networks.\nAnswer:",
            "Translate: Hello world! â†’ French\nAnswer:",
            "Is the sentiment positive or negative? Sentence: I absolutely loved it.\nAnswer:",
            "Read the passage and answer the question.\nAnswer:",
        ]
    return prompts


# ---------------------------
# Representation collection (Eq.4)
# ---------------------------
def collect_hidden_states(model, tokenizer, prompts: List[str], pad_len: int) -> torch.Tensor:
    """
    å›å‚³ shape: (num_layers, hidden_dim)
    åšæ³•ï¼šå°æ¯å€‹ prompt forwardï¼ˆä¸ä½¿ç”¨ chat æ¨¡æ¿ï¼‰ï¼Œå–æ¯å±¤æœ€å¾Œä¸€å€‹ token çš„ hiddenï¼Œ
    å†å¹³å‡ï¼ˆEq.4ï¼‰ã€‚
    """
    states = []
    # é€™è£¡åªèƒ½ç”¨ no_gradï¼ˆä¸è¦ç”¨ inference_modeï¼‰ï¼Œé¿å… peft load_adapter è§¸ç™¼ requires_grad éŒ¯èª¤
    with torch.no_grad():
        for text in prompts:
            enc = tokenizer(
                text,
                padding="max_length",
                truncation=True,
                max_length=pad_len,
                add_special_tokens=False,
                return_tensors="pt",
            ).to(model.device)

            out = model(
                **enc,
                output_hidden_states=True,
                return_dict=True,
            )
            # list[ (1, seq, dim) ] per layer â†’ å–æœ€å¾Œä¸€å€‹ token çš„ hidden æ‹¼æˆ (num_layers, dim)
            layer_vecs = torch.stack([h[:, -1, :] for h in out.hidden_states]).squeeze(1)
            states.append(layer_vecs)

    return torch.stack(states, dim=0).mean(dim=0)  # (layers, dim)


# ---------------------------
# SVD factorization of delta (CPU float32)
# ---------------------------
def svd_factorize_delta(delta: torch.Tensor, r: int):
    """
    delta: (out_features, in_features)  (B @ A)
    ç›®æ¨™é‚„åŸæˆï¼š
      B* = U_r @ sqrt(S_r)           -> (out, r)
      A* = sqrt(S_r) @ Vh_r          -> (r, in)
    æ³¨æ„ï¼šè·‘åœ¨ CPU + float32ï¼Œé¿å… Half çš„ svd ä¸æ”¯æ´ã€‚
    """
    dev = delta.device
    delta_cpu = delta.to(torch.float32, copy=True).cpu()

    # torch.linalg.svd æ¯” torch.svd ç©©å®š
    U, S, Vh = torch.linalg.svd(delta_cpu, full_matrices=False)
    r_use = min(r, U.shape[1], Vh.shape[0], S.shape[0])

    Ur = U[:, :r_use]
    Sr = S[:r_use]
    Vhr = Vh[:r_use, :]

    Sr_sqrt = torch.sqrt(Sr)
    # (out, r_use)
    B_star = Ur @ torch.diag(Sr_sqrt)
    # (r_use, in)
    A_star = torch.diag(Sr_sqrt) @ Vhr

    # è‹¥ r_use < rï¼Œéœ€è¦ zero-pad åˆ° LoRA rank å¤§å°
    if r_use < r:
        out_f, in_f = delta.shape
        B_pad = torch.zeros((out_f, r), dtype=B_star.dtype)
        A_pad = torch.zeros((r, in_f), dtype=A_star.dtype)
        B_pad[:, :r_use] = B_star
        A_pad[:r_use, :] = A_star
        B_star, A_star = B_pad, A_pad

    return B_star.to(dev), A_star.to(dev)  # æ³¨æ„ï¼šA_star shape = (r, in)


# ---------------------------
# Main
# ---------------------------
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--base_model",   type=str, required=True)
    parser.add_argument("--adapters_root", type=str, required=True)
    parser.add_argument("--data_root",    type=str, default="datasets")
    parser.add_argument("--output_dir",   type=str, default="fused_recall")
    parser.add_argument("--tasks", nargs="*", default=["sst2", "squad2", "iwslt2017-en-fr", "race", "medmcqa"])
    parser.add_argument("--samples_per_task", type=int, default=20, help="å–æ¯å€‹ task å¹¾æ¢æ¨£æœ¬ä¾†å°é½Šè¡¨ç¤º")
    parser.add_argument("--pad_len", type=int, default=128, help="æ”¶é›† hidden çš„å›ºå®šé•·åº¦")
    args = parser.parse_args()

    print("\n========== [STEP 2] RECALL merge (layer-wise similarity + softmax + SVD) ==========\n")

    # 1) æƒæ LoRA adapters
    adapter_dirs = sorted([
        os.path.join(args.adapters_root, d)
        for d in os.listdir(args.adapters_root)
        if os.path.isdir(os.path.join(args.adapters_root, d))
           and os.path.exists(os.path.join(args.adapters_root, d, "adapter_config.json"))
    ])
    assert adapter_dirs, "âŒ No LoRA adapters found in --adapters_root."

    print(f"ğŸ” Found {len(adapter_dirs)} adapters:")
    for ad in adapter_dirs:
        print(f"   â€¢ {ad}")

    # 2) tokenizerï¼ˆé—œé–‰ chat æ¨¡å¼ï¼‰ï¼Œä½¿ç”¨ base_model çš„ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(args.base_model, use_fast=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    tokenizer.padding_side = "left"  # decoder-only å»ºè­°å·¦ pad
    if hasattr(tokenizer, "chat_template"):
        tokenizer.chat_template = None
    if hasattr(tokenizer, "apply_chat_template"):
        tokenizer.apply_chat_template = lambda messages, **kwargs: "".join([m["content"] for m in messages])

    # 3) base modelï¼ˆä¸è¼‰ LoRAï¼Œç¨å¾Œå‹•æ…‹å¥—ï¼‰
    base = AutoModelForCausalLM.from_pretrained(
        args.base_model,
        torch_dtype=torch.float16,
        device_map="auto",
    )
    base.eval()

    # 4) å…ˆæŠŠç¬¬ä¸€å€‹ adapter ç•¶ anchor (a0)
    print(f"\nğŸ“Œ Load anchor adapter as 'a0': {adapter_dirs[0]}")
    peft_model = PeftModel.from_pretrained(base, adapter_dirs[0], adapter_name="a0")
    names = ["a0"]

    # 5) è’é›† promptsï¼ˆæ¯å€‹ task å– K æ¢ï¼‰
    prompts = sample_prompts_from_tasks(args.data_root, args.tasks, args.samples_per_task)
    print(f"ğŸ§¾ Using {len(prompts)} prompts for representation alignment (Eq.4)")

    # 6) Anchor è¡¨ç¤º
    anchor_rep = collect_hidden_states(peft_model, tokenizer, prompts, args.pad_len)  # (L, D)

    # 7) è¼‰å…¥å…¶ä»– adaptersï¼Œé€ä¸€æ”¶é›†è¡¨ç¤º
    all_reps = [anchor_rep]  # list of (L, D)
    for i, adir in enumerate(adapter_dirs[1:], start=1):
        print(f"ğŸ“Œ Load adapter a{i}: {adir}")
        # æ³¨æ„ï¼šload_adapter éç¨‹ä¸èƒ½åœ¨ inference_mode å…§ï¼Œå¦å‰‡æœƒå ± requires_grad çš„éŒ¯
        peft_model.load_adapter(adir, adapter_name=f"a{i}")
        names.append(f"a{i}")
        rep = collect_hidden_states(peft_model, tokenizer, prompts, args.pad_len)
        all_reps.append(rep)

    reps = torch.stack(all_reps, dim=0)  # (N, L, D)
    num_adapters, num_layers, _ = reps.shape
    print(f"âœ… reps shape = {tuple(reps.shape)} (adapters, layers, hidden_dim)")

    # 8) é€å±¤ similarityï¼ˆä»¥ anchor å°å…¶ä»– adapterï¼‰ï¼Œsoftmax æˆæ¬Šé‡ï¼ˆEq.4/5/6ï¼‰
    print("\nğŸ“ Computing layer-wise cosine similarity â†’ softmax weights...")
    anchor = reps[0]                          # (L, D)
    sim = F.cosine_similarity(anchor.unsqueeze(0), reps, dim=-1)  # (N, L)
    weights_layerwise = F.softmax(sim, dim=0)                     # (N, L)
    # å¦å¤–æº–å‚™ä¸€å€‹ã€Œadapter çš„ scalar æ¬Šé‡ã€ä½œå¾Œå‚™ï¼ˆæ‰¾ä¸åˆ°å±¤è™Ÿæ™‚ç”¨ï¼‰
    weights_scalar = weights_layerwise.mean(dim=1)                # (N,)

    # 9) å…ˆå»ºç«‹ä¸€å€‹ã€Œrecallã€adapter çµæ§‹ï¼Œç­‰æœƒè¦†å¯«å…¶ A/B æ¬Šé‡
    peft_model.load_adapter(adapter_dirs[0], adapter_name="recall")

    # 10) é‡å°æ¯ä¸€å€‹ LoRA moduleï¼Œåšã€Œå±¤å°æ‡‰ â†’ æ¬Šé‡åŠ æ¬Š â†’ SVD å›å¡«ã€
    print("\nğŸ§  Performing layer-wise Î”W merge with SVD (per LoRA module)...")
    with torch.no_grad():
        for name, module in peft_model.named_modules():
            if not (hasattr(module, "lora_A") and hasattr(module, "lora_B")):
                continue
            if "recall" not in module.lora_A or "recall" not in module.lora_B:
                continue

            # å˜—è©¦å°æ‡‰åˆ° transformer å±¤ï¼Œç”¨è©²å±¤çš„æ¬Šé‡ï¼›å¦å‰‡ç”¨ scalar æ¬Šé‡
            layer_id = get_layer_id_from_name(name)
            if 0 <= layer_id < num_layers:
                w = weights_layerwise[:, layer_id]  # (N,)
            else:
                w = weights_scalar                   # (N,)

            # æ‰€æœ‰ adapter çš„ delta ç–ŠåŠ 
            deltas = []
            r_here = None
            for a_name in names:
                A = module.lora_A[a_name].weight    # (r, in)
                B = module.lora_B[a_name].weight    # (out, r)
                if r_here is None:
                    r_here = A.shape[0]
                deltas.append(B @ A)                 # (out, in)
            # åŠ æ¬Š
            delta_sum = torch.zeros_like(deltas[0])
            for q in range(num_adapters):
                delta_sum.add_(deltas[q] * w[q])

            # SVD åˆ†è§£ â†’ å¾—åˆ°æ–°çš„ B*, A* ï¼ˆæ³¨æ„ A* shape æ˜¯ (r,in)ï¼‰
            B_star, A_star = svd_factorize_delta(delta_sum, r_here)

            # å›å¡«åˆ°ã€Œrecallã€adapter
            module.lora_A["recall"].weight.data.copy_(A_star)  # (r, in)
            module.lora_B["recall"].weight.data.copy_(B_star)  # (out, r)

    # 11) è¨­å®š active adapter â†’ merge åˆ° base æ¬Šé‡ä¸¦å„²å­˜å®Œæ•´æ¨¡å‹ï¼ˆå« config.json / model.safetensorsï¼‰
    peft_model.set_adapter("recall")
    merged = peft_model.merge_and_unload()

    os.makedirs(args.output_dir, exist_ok=True)
    merged.save_pretrained(args.output_dir)
    tokenizer.save_pretrained(args.output_dir)

    print(f"\nğŸ‰ RECALL model fused â†’ {args.output_dir}\n")


if __name__ == "__main__":
    main()

'''
# merge_recall.py  â€” RECALL (layer-wise, similarity-guided) merging
import os, re, glob, json, math, argparse, random
from typing import List, Dict, Tuple
import torch
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# -------------- utils: name parsing -----------------
_LAYER_PAT = re.compile(r"(layers|h)\.(\d+)\.")  # Qwen/LLaMA: model.layers.{i}.xxx

def extract_layer_idx(name: str) -> int:
    m = _LAYER_PAT.search(name)
    return int(m.group(2)) if m else -1

def set_seed(seed: int = 1337):
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

# -------------- dataset probing ---------------------
def load_probe_inputs(datasets_root: str, num_probe: int) -> List[str]:
    """
    æƒæ datasets_root/*/train.jsonl æˆ– validation.jsonlï¼Œæ“·å– 'input' æ¬„ä½
    """
    files = []
    for sub in os.listdir(datasets_root):
        d = os.path.join(datasets_root, sub)
        if not os.path.isdir(d): 
            continue
        for fname in ["train.jsonl", "validation.jsonl", "val.jsonl"]:
            p = os.path.join(d, fname)
            if os.path.exists(p):
                files.append(p)
    pool = []
    for fp in files:
        with open(fp, "r", encoding="utf-8") as f:
            for line in f:
                try:
                    j = json.loads(line)
                    if "input" in j and isinstance(j["input"], str):
                        pool.append(j["input"])
                except:
                    pass
    random.shuffle(pool)
    pool = pool[:num_probe] if num_probe > 0 else pool
    if not pool:
        raise RuntimeError(f"No probe inputs found under {datasets_root}. Put jsonl with 'input' field.")
    # çµ±ä¸€åœ¨æœ«å°¾åŠ  "Answer:"ï¼ˆèˆ‡ä½ çš„ SFT prompt å°é½Šï¼‰
    return [x.rstrip() + "\nAnswer:" for x in pool]

# -------------- hidden states collection -------------
@torch.no_grad()
def collect_layer_reps(
    peft_model: PeftModel,
    adapter_name: str,
    tokenizer,
    probe_inputs: List[str],
    device: str = "cuda",
    max_len: int = 512,
    batch_size: int = 8,
) -> Dict[int, torch.Tensor]:
    """
    å•Ÿç”¨æŸå€‹ adapterï¼Œå° probe_inputs åšå‰å‘ï¼Œæ”¶é›†æ¯å±¤ hidden_states çš„ mean-pooled è¡¨å¾µ
    å›å‚³ï¼š {layer_idx -> vector (hidden_dim,)}
    """
    peft_model.set_adapter(adapter_name)
    peft_model.eval()

    reps_sum: Dict[int, torch.Tensor] = {}
    reps_cnt: Dict[int, int] = {}

    for i in range(0, len(probe_inputs), batch_size):
        batch = probe_inputs[i:i+batch_size]
        enc = tokenizer(
            batch,
            padding=True,
            truncation=True,
            max_length=max_len,
            return_tensors="pt",
            add_special_tokens=True,
        ).to(device)
        out = peft_model.base_model(
            input_ids=enc.input_ids,
            attention_mask=enc.attention_mask,
            output_hidden_states=True,
            use_cache=False,
        )
        # out.hidden_states: (layers+1) list of [B, T, H]ï¼ˆå« embedding 0 å±¤ï¼‰
        hs = out.hidden_states  # List[Tensor]
        for layer_idx, h in enumerate(hs[1:], start=0):  # å»æ‰ embedding å±¤ï¼Œå¾ 0 å°æ‡‰ç¬¬ä¸€å€‹ block å‰è¼¸å‡º
            # mean over tokens then mean over batch â†’ [H]
            # åªå°é padding çš„ä½ç½®åšå¹³å‡
            mask = enc.attention_mask.unsqueeze(-1).float()  # [B, T, 1]
            masked = h * mask
            denom = mask.sum(dim=1).clamp_min(1e-6)  # [B, 1]
            pooled = (masked.sum(dim=1) / denom)  # [B, H]
            vec = pooled.mean(dim=0)  # [H]
            if layer_idx not in reps_sum:
                reps_sum[layer_idx] = vec.clone()
                reps_cnt[layer_idx] = 1
            else:
                reps_sum[layer_idx] += vec
                reps_cnt[layer_idx] += 1

    reps_mean = {i: (reps_sum[i] / reps_cnt[i]) for i in reps_sum}
    return reps_mean  # {layer_idx: [H]}

def cosine_sim(a: torch.Tensor, b: torch.Tensor) -> float:
    a = F.normalize(a.float(), dim=0)
    b = F.normalize(b.float(), dim=0)
    return float((a * b).sum().item())

def softmax(x: torch.Tensor, tau: float = 10.0) -> torch.Tensor:
    z = (x * tau) - (x.max())
    exp = torch.exp(z)
    return exp / exp.sum().clamp_min(1e-8)

# -------------- layer-wise weight computation --------
def compute_layerwise_weights(reps_per_adapter: Dict[str, Dict[int, torch.Tensor]],
                              temperature: float = 10.0) -> Dict[int, Dict[str, float]]:
    """
    çµ¦å®šï¼šæ¯å€‹ adapter çš„å„å±¤è¡¨å¾µ
      reps_per_adapter: {adapter -> {layer_idx -> vec}}
    ç”¢å‡ºï¼šé€å±¤ softmax æ¬Šé‡
      weights[layer_idx][adapter] = w
    åšæ³•ï¼šä»¥ã€Œå„é©é…å™¨è¡¨å¾µçš„å‡å€¼ï¼ˆcentroidï¼‰ã€ç‚ºåƒè€ƒï¼Œç®— cosineï¼Œç›¸ä¼¼åº¦è¶Šé«˜ï¼Œæ¬Šé‡è¶Šå¤§ã€‚
    """
    adapters = sorted(reps_per_adapter.keys())
    # çµ±ä¸€å±¤é›†åˆ
    all_layers = sorted(set().union(*[set(reps_per_adapter[a].keys()) for a in adapters]))
    weights: Dict[int, Dict[str, float]] = {}

    for li in all_layers:
        vecs = [reps_per_adapter[a][li] for a in adapters if li in reps_per_adapter[a]]
        if len(vecs) != len(adapters):
            # æŸäº› adapter å°‘å±¤ï¼ˆç†è«–ä¸Šä¸æœƒï¼‰ï¼Œç”¨ 0 æ¬Šé‡
            sims = torch.zeros(len(adapters))
            w = softmax(sims, tau=temperature)
            weights[li] = {a: float(w[j].item()) for j, a in enumerate(adapters)}
            continue

        centroid = torch.stack(vecs, dim=0).mean(dim=0)
        sims = torch.tensor([cosine_sim(reps_per_adapter[a][li], centroid) for a in adapters])
        w = softmax(sims, tau=temperature)
        weights[li] = {a: float(w[j].item()) for j, a in enumerate(adapters)}

    return weights  # {layer_idx: {adapter: w}}

# -------------- LoRA delta extraction & merging ------
def _get_lora_delta(module, adapter_name: str) -> torch.Tensor:
    """
    å›å‚³è©² module åœ¨ adapter ä¸‹çš„ LoRA delta:  B @ A * scaling
    åªæ”¯æ´ Linear é¡å‹çš„ LoRAï¼ˆq/k/v/o ç­‰ï¼‰
    """
    # peft çš„ LoRA å±¤æœƒæ›é€™äº› dict å±¬æ€§
    A = module.lora_A[adapter_name].weight   # [r, in]
    B = module.lora_B[adapter_name].weight   # [out, r]
    r = A.shape[0]
    alpha = module.lora_alpha[adapter_name]
    scaling = alpha / r
    delta = torch.matmul(B, A) * scaling     # [out, in]
    return delta

@torch.no_grad()
def merge_layerwise(
    peft_model: PeftModel,
    layerwise_w: Dict[int, Dict[str, float]],
    out_dir: str,
):
    """
    ä¾æ“šæ¯å±¤æ¬Šé‡ï¼ŒæŠŠå„ adapter çš„ LoRA delta åŠ æ¬Šå¾ŒåŠ é€²åº•å±¤ Linear æ¬Šé‡
    """
    base = peft_model.base_model
    device = next(base.parameters()).device
    dtype = next(base.parameters()).dtype

    # éæ­·æ‰€æœ‰å…· LoRA çš„ module
    for name, mod in peft_model.named_modules():
        if not hasattr(mod, "lora_A"):
            continue  # é LoRA åŒ–æ¨¡çµ„
        layer_idx = extract_layer_idx(name)
        if layer_idx < 0:
            # å¦‚æœæŠ“ä¸åˆ°å±¤è™Ÿï¼šä¿å®ˆç”¨å¹³å‡æ¬Šé‡
            adapters = list(mod.lora_A.keys())
            avg_w = 1.0 / max(1, len(adapters))
            w_map = {a: avg_w for a in adapters}
        else:
            # è©²å±¤çš„æ¬Šé‡è¡¨
            w_map = layerwise_w.get(layer_idx, None)
            if w_map is None:
                adapters = list(mod.lora_A.keys())
                avg_w = 1.0 / max(1, len(adapters))
                w_map = {a: avg_w for a in adapters}

        # é€ adapter å– deltaï¼ŒåŠ æ¬Šç›¸åŠ 
        delta_sum = None
        for a_name in mod.lora_A.keys():
            if a_name not in w_map:
                continue
            w = w_map[a_name]
            d = _get_lora_delta(mod, a_name).to(device=device, dtype=dtype)
            delta_sum = d * w if delta_sum is None else delta_sum + d * w

        # æŠŠåŠ æ¬Šå¾Œçš„ delta åˆé€²åº•å±¤ Linear æ¬Šé‡
        if delta_sum is not None:
            # Linear æ¬Šé‡åç¨±é€šå¸¸åœ¨ mod.base_layer.weight
            if hasattr(mod, "weight") and mod.weight is not None:
                # æŸäº› PEFT ç‰ˆæœ¬ç›´æ¥æŠŠ Linear åŒ…æˆ LoRALinearï¼Œæ¬Šé‡å°±æ˜¯ mod.weight
                mod.weight += delta_sum
            elif hasattr(mod, "base_layer") and hasattr(mod.base_layer, "weight"):
                mod.base_layer.weight += delta_sum
            else:
                # ç„¡æ³•æ‰¾åˆ°åº•å±¤æ¬Šé‡ï¼ˆç†è«–ä¸Šä¸æœƒï¼‰
                pass

    # ç§»é™¤ LoRA çµæ§‹ä¸¦ä¿å­˜ç´”æ¬Šé‡
    # æœ€ç©©çš„ä½œæ³•ï¼šç›´æ¥æŠŠ peft çš„ lora çµæ§‹ä¿ç•™ä½†ä¸å†éœ€è¦ï¼›æˆ‘å€‘æŠŠåº•å±¤å·²ç¶“åŠ å¥½ delta
    # ç”¨ base_model.save_pretrained è¼¸å‡ºç´”åº•åº§
    os.makedirs(out_dir, exist_ok=True)
    base.save_pretrained(out_dir)
    print(f"ğŸ’¾ Saved merged (pure) model to: {out_dir}")

# -------------- main --------------------------------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--base_model", required=True)
    ap.add_argument("--adapters_root", default="checkpoints_full")
    ap.add_argument("--adapters", nargs="*", default=None)
    ap.add_argument("--datasets_root", default="./datasets")
    ap.add_argument("--num_probe", type=int, default=200)
    ap.add_argument("--temperature", type=float, default=10.0)
    ap.add_argument("--tokenizer", default=None)  # è‹¥ä¸çµ¦ï¼Œæ²¿ç”¨ base çš„ tokenizer
    ap.add_argument("--dtype", default="bfloat16", choices=["bfloat16","float16","float32"])
    ap.add_argument("--out_dir", default="fused_recall_true")
    ap.add_argument("--seed", type=int, default=1337)
    args = ap.parse_args()

    set_seed(args.seed)
    torch_dtype = {"bfloat16": torch.bfloat16, "float16": torch.float16, "float32": torch.float32}[args.dtype]

    # 1) æ”¶é›† adapters
    if args.adapters is None:
        cands = sorted([p for p in glob.glob(os.path.join(args.adapters_root, "*")) if os.path.isdir(p)])
    else:
        cands = args.adapters
    adapters = []
    for p in cands:
        if os.path.exists(os.path.join(p, "adapter_config.json")):
            adapters.append(p)
    if len(adapters) < 2:
        raise RuntimeError("Need at least two adapters for merging.")

    print("âœ… Adapters:")
    for i, p in enumerate(adapters):
        print(f"  [{i}] {p}")

    # 2) è¼‰ base + ç¬¬ä¸€å€‹ LoRA ç•¶ anchorï¼Œå¾ŒçºŒè¼‰é€² peft
    print(f"ğŸ”„ Loading base: {args.base_model}")
    base = AutoModelForCausalLM.from_pretrained(args.base_model, torch_dtype=torch_dtype, device_map="auto")
    peft_model = PeftModel.from_pretrained(base, adapters[0])
    peft_model.set_adapter("a0")

    # å¾ŒçºŒ adapters
    for idx, adir in enumerate(adapters[1:], start=1):
        name = f"a{idx}"
        peft_model.load_adapter(adir, adapter_name=name)

    # 3) æº–å‚™ tokenizer èˆ‡ probe inputs
    tok_id = args.tokenizer if args.tokenizer else args.base_model
    tokenizer = AutoTokenizer.from_pretrained(tok_id, use_fast=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    probe_inputs = load_probe_inputs(args.datasets_root, args.num_probe)
    print(f"ğŸ§ª Loaded {len(probe_inputs)} probe prompts from {args.datasets_root}")

    # 4) é€ adapter æ”¶é›†å„å±¤è¡¨å¾µ
    reps_per_adapter: Dict[str, Dict[int, torch.Tensor]] = {}
    for idx in range(len(adapters)):
        aname = f"a{idx}"
        reps = collect_layer_reps(peft_model, aname, tokenizer, probe_inputs)
        reps_per_adapter[aname] = reps
        print(f"   Â· collected reps for {aname} ({len(reps)} layers)")

    # 5) é€å±¤è¨ˆç®— softmax æ¬Šé‡
    layer_w = compute_layerwise_weights(reps_per_adapter, temperature=args.temperature)
    # ç°¡è¦å°å‡ºå¹¾å±¤æ¬Šé‡
    some_layers = sorted(layer_w.keys())[:5] + sorted(layer_w.keys())[-5:]
    print("ğŸ“Š Sample layer weights:")
    for li in some_layers:
        wm = layer_w[li]
        pretty = ", ".join([f"{k}:{wm[k]:.2f}" for k in sorted(wm.keys())])
        print(f"  layer {li}: {pretty}")

    # 6) é€å±¤æŠŠ LoRA delta åŠ æ¬Šåˆä½µåˆ°åº•å±¤
    merge_layerwise(peft_model, layer_w, args.out_dir)

if __name__ == "__main__":
    main()
'''