#!/usr/bin/env bash
set -euo pipefail

# =============================
# RECALL full pipeline (4090 single-GPU, QLoRA)
# - base: Qwen/Qwen2-7B-Instruct (ç­‰åƒ¹æ–¼ LLaMA-2-7B-Chat çš„ chat/instructåŸºç¤)
# - å¯ç”¨ N_SAMPLES æ§åˆ¶æ¯å€‹ task å–å¤šå°‘ç­† (0 è¡¨ç¤ºå…¨é‡)
# - epochs/batch ä¾ Table 5 èª¿æ•´ï¼ˆå†ç¶“å–®å¡åŒ–ï¼šper_device_batch=1 + GA ç´¯ç©åœ¨ train_single_task.py å…§è™•ç†ï¼‰
# =============================

# ---- å¯èª¿åƒæ•¸ ----
#BASE_MODEL="Qwen/Qwen2-7B-Instruct"
#OUT_DIR="checkpoints_recall"
#FUSED_DIR="fused_recall_qwen2"
BASE_MODEL="./Llama-2-7b-chat-hf"
FUSED_DIR="fused_recall_llama2_1000"
OUT_DIR="checkpoints_recall_llama_1000"
DATA_ROOT="./datasets"                  # ä½ çš„ jsonl æ‰€åœ¨æ ¹ç›®éŒ„
RESULTS_DIR="results_eval_500"
LOG_DIR="logs"

# æ§åˆ¶æ¯å€‹ task è¨“ç·´åªæŠ½éƒ¨åˆ†è³‡æ–™ï¼ˆ0=å…¨é‡ï¼‰
N_SAMPLES="${N_SAMPLES:-1000}"           # ä¾‹ï¼šexport N_SAMPLES=200

# ç”Ÿæˆ/è©•ä¼°é•·åº¦ (ä¿å®ˆé¿å… OOM)
MAX_LEN_TRAIN=384
MAX_LEN_EVAL=512
MAX_NEW_TOKENS=64

# ä»»å‹™æ¸…å–®ï¼ˆè«–æ–‡äº”å€‹ï¼‰
TASKS=("sst2" "squad2" "iwslt2017" "race" "medmcqa")

mkdir -p "$OUT_DIR" "$FUSED_DIR" "$RESULTS_DIR" "$LOG_DIR"

TIME_TAG=$(date +%Y%m%d_%H%M)
LOG_FILE="$LOG_DIR/run_${TIME_TAG}.log"

echo "========== [STEP 0] Env ==========" | tee -a "$LOG_FILE"
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
export HF_HUB_DISABLE_TELEMETRY=1
# ï¼ˆå¯é¸ï¼‰ä½¿ç”¨ Flash-Attn 2 æ™‚å¸¸è¦‹ï¼šexport CUDA_VISIBLE_DEVICES=0

# -------------------------------
# æ¯å€‹ task çš„è«–æ–‡è¨­å®šï¼ˆTable 5ï¼‰ï¼Œå–®å¡åŒ–æ–¹æ¡ˆï¼š
#   per_device_batch å›ºå®š 1ï¼›GA åœ¨ train_single_task.py å…§æŒ‰ task è‡ªå‹•è¨­å®š
#   é€™é‚Šåªå‚³ epochs + subsetï¼ˆN_SAMPLESï¼‰
# -------------------------------
epochs_for() {
  case "$1" in
    sst2) echo 3;;
    squad2) echo 4;;
    medmcqa) echo 3;;
    race) echo 5;;
    iwslt2017) echo 5;;
    *) echo 3;;
  esac
}

echo "========== [STEP 1] Train each task (QLoRA, subset=${N_SAMPLES}) ==========" | tee -a "$LOG_FILE"

for TASK in "${TASKS[@]}"; do
  CKPT_DIR="$OUT_DIR/$TASK"
  if [[ -f "$CKPT_DIR/adapter_config.json" ]]; then
    echo "âœ… Skip $TASK (exists: $CKPT_DIR)" | tee -a "$LOG_FILE"
    continue
  fi

  EPOCHS="$(epochs_for "$TASK")"
  echo "ğŸš€ Training $TASK (epochs=$EPOCHS, subset=$N_SAMPLES) ..." | tee -a "$LOG_FILE"

  # èªªæ˜ï¼š
  # - éœ€è¦ä½ ä½¿ç”¨ã€Œæˆ‘çµ¦çš„ QLoRA ç‰ˆ train_single_task.pyã€ï¼ˆæ”¯æ´ --load_in_4bit / --max_len / --subset / --epochsï¼‰
  # - å…§éƒ¨æœƒä¾ task è‡ªå‹•è¨­ç½® LoRA r/alpha/dropout & GAï¼Œä»¥ç¬¦åˆè«–æ–‡ Table 5 çš„ç­‰æ•ˆ batch
  python train_single_task.py \
    --task "$TASK" \
    --base_model "$BASE_MODEL" \
    --output_dir "$OUT_DIR" \
    --subset "$N_SAMPLES" \
    --epochs "$EPOCHS" \
    --load_in_4bit \
    --max_len "$MAX_LEN_TRAIN" 2>&1 | tee -a "$LOG_FILE"
done

echo "âœ… æ‰€æœ‰LoRA trainingå®Œæˆ" | tee -a "$LOG_FILE"

# safetensors -> binï¼ˆè‹¥éœ€è¦ï¼‰
echo "========== [STEP 1.5] Ensure LoRA bin ==========" | tee -a "$LOG_FILE"
python - <<'PY'
import os, torch, glob
from safetensors.torch import load_file
root="checkpoints_recall"
for ad in glob.glob(os.path.join(root, "*")):
    if not os.path.isdir(ad): continue
    st=os.path.join(ad,"adapter_model.safetensors")
    bn=os.path.join(ad,"adapter_model.bin")
    if os.path.exists(st) and not os.path.exists(bn):
        print(f"âš™ï¸  convert {st} -> {bn}")
        torch.save(load_file(st), bn)
print("done.")
PY

# ------------------------------- 
# RECALL mergeï¼ˆé€å±¤ similarity + softmaxï¼‰ 
# * éœ€è¦ã€Œæˆ‘çš„ merge_recall.pyï¼ˆlayer-wise similarity + softmaxï¼‰ã€ç‰ˆæœ¬ 
# * ç‚ºé¿å… hidden é•·åº¦ä¸é½Šå°è‡´ stack errorï¼Œé€™è£¡å›ºå®š merge æ™‚çš„æ”¶é›†é•·åº¦ 
# ------------------------------- 
echo "========== [STEP 2] RECALL merge (layer-wise similarity + softmax) ==========" | tee -a "$LOG_FILE" 
# å›ºå®šæ”¶é›† hidden çš„ prompt é•·åº¦ï¼Œé¿å…å †ç–Šç¶­åº¦ä¸ä¸€è‡´ 
export RECALL_MERGE_PADLEN="${RECALL_MERGE_PADLEN:-128}" 
# æ”¶é›†æ¯å€‹ task çš„å°æ¨£æœ¬ä½œç‚ºè¡¨ç¤ºå°é½Šï¼ˆæ¯ task 20 æ¢å³å¯ï¼‰ 
export RECALL_MERGE_SAMPLES_PER_TASK="${RECALL_MERGE_SAMPLES_PER_TASK:-20}" 

#python merge_recall.py \ 
# --base_model "$BASE_MODEL" \ 
# --adapters_root "$OUT_DIR" \ 
# --data_root "$DATA_ROOT" \ 
# --output_dir "$FUSED_DIR" 2>&1 | tee -a "$LOG_FILE" 

python merge_recall.py \
  --base_model "$BASE_MODEL" \
  --adapters_root "$OUT_DIR" \
  --data_root "$DATA_ROOT" \
  --output_dir "$FUSED_DIR" \
  --samples_per_task 20 \
  --pad_len 128 \
# -------------------------------
# Evaluateï¼ˆé—œé–‰ chat æ¨¡æ¿ï¼Œé€ task EM/Acc/BLEUï¼‰
# -------------------------------
echo "========== [STEP 3] Evaluate fused model ==========" | tee -a "$LOG_FILE"

python evaluate_all_tasks.py \
  --model "$FUSED_DIR" \
  --base_model "$BASE_MODEL" \
  --data_root "$DATA_ROOT" \
  --results_dir "$RESULTS_DIR" \
  --max_examples 500 \
  --max_src_len "$MAX_LEN_EVAL" \
  --max_new_tokens "$MAX_NEW_TOKENS" 2>&1 | tee -a "$LOG_FILE"

# -------------------------------
# Summary
# -------------------------------
echo "========== [STEP 4] Summarize ==========" | tee -a "$LOG_FILE"
python analyze_results.py 2>&1 | tee -a "$LOG_FILE"

echo "ğŸ‰ RECALL å®Œæˆ" | tee -a "$LOG_FILE"
echo "ğŸ“ Outputs:" | tee -a "$LOG_FILE"
echo "   - $OUT_DIR/" | tee -a "$LOG_FILE"
echo "   - $FUSED_DIR/" | tee -a "$LOG_FILE"
echo "   - $RESULTS_DIR/" | tee -a "$LOG_FILE"
echo "   - $LOG_FILE" | tee -a "$LOG_FILE"
